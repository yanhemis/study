# 🧠 자연어 처리 정리 (13주차 수업 + 질문 기반 심화 요약)

---

## 📌 1. 자연어 처리(NLP) 개요
- 자연어 처리(NLP)는 인간의 언어를 컴퓨터가 이해하고 처리하게 만드는 기술
- 응용 분야: 감성 분석, 스팸 필터링, 챗봇, 번역, 요약 등
- 오늘 수업의 예시: IMDb 감성 분류 (긍정/부정)

---

## 📌 2. 전처리 과정

### 🔸 2.1 토큰화(Tokenization)
- 문장을 의미 단위(토큰)로 분리
- 단어 단위, 문장 단위, 형태소 단위 등 다양
- 사용 도구:
  - `nltk.word_tokenize`: 일반적인 영어 문장 처리
  - `WordPunctTokenizer`: 구두점도 별도로 분리
  - `text_to_word_sequence`: keras 기반, 특수문자 제거 중심
  - `TreebankTokenizer`: Penn Treebank 규칙 기반

> ✅ **질문 반영**: `CountVectorizer` 같은 함수는 내부적으로 기본적인 토큰화를 수행하지만,  
> 정교한 제어가 필요할 때는 개발자가 별도로 `nltk`, `KoNLPy`, `regex` 등을 사용해 토큰화를 진행함.

---

### 🔸 2.2 정제(Cleaning)와 정규화(Normalization)
- 정제: 특수문자 제거, 소문자화, 불필요한 기호 제거
- 정규화:
  - 표제어 추출 (Lemmatization)
  - 어간 추출 (Stemming)
  - 대소문자 통합
  - 불용어 제거

---

## 📌 3. 벡터화(Vectorization)

### 🔸 3.1 BoW (Bag of Words)
- 단어 순서 무시, 빈도 기반 수치 표현
- `CountVectorizer`로 구현
- 한계: 희소성(sparsity), 의미 무시

### 🔸 3.2 TF-IDF
- 자주 등장하지만 중요하지 않은 단어에 낮은 가중치
- 희귀하지만 특정 문서에서 자주 등장한 단어에 높은 가중치
- `TfidfVectorizer`로 구현

> ✅ **질문 반영**: 수업에서 CountVectorizer를 직접 썼기 때문에 토큰화는 함수 내부에서 처리됨.  
> 따라서 **"토큰화를 생략했다기보단 자동화된 것"**임.

---

## 📌 4. 대형 플랫폼 vs 소형 플랫폼

### 🔸 대형 플랫폼 (Meta, Google 등)
- 자체 subword 기반 토크나이저 개발
- 주로 BPE(Byte Pair Encoding), SentencePiece 기반
- 의미 파악은 Transformer 임베딩에서 처리

### 🔸 소형 플랫폼
- HuggingFace의 공개 모델 + 토크나이저 조합 사용
- 예: KoBERT, XLM-RoBERTa + AutoTokenizer
- 한국어: KoNLPy 형태소 분석기 + 사용자 사전

> ✅ **질문 반영**: 대형 플랫폼은 코드 스위칭/다국어 처리를 위해 **subword 기반** 토크나이저를 사용함.  
> 소형 플랫폼은 성능보다는 **도구 조합과 비용 효율**에 집중함.

---

## 📌 5. Subword 토크나이저의 한계와 관계 정의

### 🔸 Subword 토크나이저는
- 철자 기반 분할을 수행
- 의미 관계를 직접 담지 않음

### 🔸 의미 관계는 어디서 정의되는가?
- Embedding layer (ex. Word2Vec, FastText, BERT)
- Transformer의 attention mechanism
- 벡터 간 거리/유사도 등을 통해 단어 간 관계를 학습

---

## 📌 6. 실습 요약 (IMDB 리뷰 분류기)
1. 텍스트 데이터 수집 → 전처리
2. BoW / TF-IDF로 벡터화
3. 로지스틱 회귀 모델 학습
4. 긍정/부정 분류

---

## 📎 오늘 질문 요약

| 질문 | 요점 요약 |
|------|-----------|
| 토큰화 vs 벡터화 차이 | 토큰화는 단어 분리, 벡터화는 수치 표현. 둘 다 필수 단계 |
| 벡터화만 쓰는 경우 | 토큰화가 함수 내부에 포함된 경우 |
| NLTK 토큰화 정확도 | 영어 기준 약 95~97%, 비표준 문장/SNS는 정확도 낮음 |
| KoNLPy 정확도 | Mecab > Komoran > Kkma > Okt 순, 한국어 실무는 Mecab 권장 |
| 대형 플랫폼 처리 방식 | 대부분 subword 토크나이저(BPE, SentencePiece) 사용 |
| 다국어 혼합문장 처리 | subword + context-aware embedding으로 해결 |
| 단어 간 관계 정의 | 토크나이저가 아닌 임베딩 모델이 처리 |
| 소형 플랫폼 대응법 | 공개 모델 조합, 형태소 분석기 활용, 사용자 사전 도입 |
